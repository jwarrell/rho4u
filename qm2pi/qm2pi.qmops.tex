\section{Interpretation of QM}
\subsection{Supporting definitions}

To provide our interpretation of quantum mechanics we need to develop
a number of supporting definitions. As the reader familiar with
process algebraic systems can readily verify, these definitions make
\emph{essential} use of the reflective operations and as such identify
this calculus as uniquely suited to this particular task.

Among these operations we find a notion of \emph{multiplication} of
names that interacts well with a notion of \emph{tensor product} of
processes. Even more intriguingly, we find a notion of a \emph{dual}
to a process in the form of maps from processes to names. While
notions of composite names have been investigated in the process
algebraic literature, it is the fact that names reflect process
structure that enables the collection of duals to enjoy an algebraic
structure dual to the collection of processes (i.e. there are
operations available to duals that reflect the operations on
processes). Moreover, it is this structure that enables an effective
definition of inner product.

\subsubsection{Multiplication}
\begin{mathpar}
  \quotep{P} \cdot \quotep{Q} := \quotep{(P\mathsf{|}Q)}
  \and equivalently
  \and x \cdot y := \quotep{(\procn{x}\mathsf{|}\procn{y})}
  \and \\
  \quotep{Q} \cdot P := P\{ \quotep{(Q\mathsf{|}R)} / \quotep{R} : \quotep{R} \in \freenames{P} \} \\
  \and equivalently \\
  \and x \cdot P := P\{ \quotep{(\procn{x}\mathsf{|}\procn{z})} / z : z \in \freenames{P} \}
\end{mathpar}

\paragraph{Discussion}
The first equation needs little explanation; the second says that each
free name of the process is replaced with the multiplication of that
name by the scalar. Multiplication of a scalar (name) by a state
(process) results in a process all the names of which have been `moved
over' by parallel composition with the process the scalar
quotes. There is a subtlety that the bound names have to be
manipulated so that multiplied names aren't accidentally
captured. Since there are many ways to achieve this we simply demand
that multiplication not accidentally capture free names.

\begin{remark}\label{rem:multiplication_identities}
  The reader is invited to verify that for all $x,y,z \in \QProc$ and $P \in \Proc$
  \begin{mathpar}
    x \cdot \quotep{0} \equiv x 
    \and
    x \cdot y \equiv y \cdot x
    \and
    x \cdot (y \cdot z) \equiv (x \cdot y) \cdot z
    \and \\
    \quotep{0} \cdot P \equiv P
    \and \\
    x \cdot (y \cdot P) \equiv (x \cdot y) \cdot P
    \and \\
    x \cdot (P|Q) \equiv (x \cdot P) | (x \cdot Q)
    \and \\
    x \cdot (P \mathsf{+} Q) = (x \cdot P) \mathsf{+} (x \cdot Q)
    \and \\
  \end{mathpar}
\end{remark}

\begin{mathpar}
  
\end{mathpar}

\subsubsection{Contexts and duality}

As mentioned previously, contexts are going to play in the role of
duals to vectors. Or in Dirac's nomenclature, $K$ plays bra to $P$'s
ket. As such, we will want something that acts like an inner product,
$K \cdot P$. Note that if names are scalars, then we expect that $K
\cdot P$ to yield a name.

\begin{mathpar}
  K \cdot P := \quotep{((K[P])\sigma_{\otimes}(K[\pzero],P))}
\end{mathpar}

where

\begin{eqnarray}
  \sigma_{\otimes}(P,Q) & := &\{w_{1} \cdot x / x, w_{2} \cdot y / y : x \in \freenames{P}, y \in \freenames{Q},  w_{1} = \mathsf{NF}(Q), w_{2} = \mathsf{NF}(P) \} \nonumber \\ 
  \mathsf{NF}(P) & := & \quotep{\Pi_{u \in \freenames{P}}\procn{u}} \nonumber
\end{eqnarray}

The definition may not seem intuitive at first. However, all that's
happening is guarding against unwarranted interaction, as we will see
shortly. In the meantime let us formally specify the duality between
contexts and processes.

\begin{mathpar}
  P^{\bot} := P\mathsf{|}\Box
  \and
  K^{\bot} := K[0]
\end{mathpar}

We can check that for all $P$ and for contexts of the form $K =
P\mathsf{|}\Box$ for some $P$ the operation $(-)^{\bot}$ is
involutive.

\begin{mathpar}
  (P^{\bot})^{\bot} = (P\mathsf{|}\Box)^{\bot} = P|0 = P
  \and
  (K^{\bot})^{\bot} = (K[0])^{\bot} = (P)^{\bot} = P\mathsf{|}\Box
\end{mathpar}

More generally, if $K$ is not of the form $P\mathsf{|}\Box$ then define

\begin{mathpar}
  K^{\bot} := K[\dropn{(\quotep{K[0]})}]
\end{mathpar}

Using the observation in remark () we note that $K^{\bot}$ has a
\emph{unique} decomposition, because $K[0]$ cannot include a name of the
form $\quotep{K[0]}$. Thus, we can provide a definition for $P^{\bot}$
that replaces $\dropn{(\quotep{K[0]})}$ with $\Box$, i.e.

\begin{mathpar}
  P^{\bot} := P\{\Box / \dropn{(\quotep{K[0]})}\}
\end{mathpar}

The reader can check that this is involutive for all $P$ and $K$. We
shall primarily be dealing with contexts of the form
$P\mathsf{|}\Box$, and so will typically use the simpler definition.

Following a similar line of reasoning we can define

\begin{mathpar}
  P^{\bot} \cdot x := (x \cdot P)^{\bot}
\end{mathpar}

Our definitions allow us to verify:

\begin{mathpar}
  P^{\bot} \cdot Q := \quotep{(P\mathsf{|}Q)\{w_{1} \cdot x / x, w_{2} \cdot y / y : x \in \freenames{P}, y \in \freenames{Q}, w_{1} = \mathsf{NF}(Q), w_{2} = \mathsf{NF}(P) \}}
\end{mathpar}

\subsubsection{Outer product}
We can define an outer product of processes

\begin{mathpar}
  P \otimes Q := (P\mathsf{|}Q)\{w_{1} \cdot x / x, w_{2} \cdot y / y : x \in \freenames{P}, y \in \freenames{Q}, w_{1} = \mathsf{NF}(Q), w_{2} = \mathsf{NF}(P) \}
\end{mathpar}

which is consistent with inner product and respects the usual identities

\begin{mathpar}
  (u \cdot P) \otimes Q = P \otimes (u \cdot Q)
\end{mathpar}

as the reader may quickly check.

\begin{eqnarray}
  (x \cdot P) \otimes Q & & \nonumber \\
  & = & (u \cdot P\mathsf{|}Q)\sigma_{\otimes}((u \cdot P),Q) \nonumber \\
  & = & (u \cdot P)\sigma_{\otimes}((u \cdot P),Q)\mathsf{|}Q\sigma_{\otimes}((u \cdot P),Q) \nonumber \\
  & = & (u \cdot P)\{ \mathsf{NF}(Q)\cdot u \cdot x\}\mathsf{|}Q\{ \mathsf{NF}(u \cdot P)\cdot y\} \nonumber \\
  & = & P\{ \mathsf{NF}(u\cdot Q) \cdot x\}\mathsf{|}(u \cdot Q)\{ \mathsf{NF}(P) \cdot (u \cdot y)\} \nonumber \\
  & = & P\sigma_{\otimes}(P,(u \cdot Q))\mathsf{|}(u \cdot Q)\sigma_{\otimes}(P,(u \cdot Q)) \nonumber \\
  & = & (P\mathsf{|}u\cdot Q)\sigma_{\otimes}(P,(u \cdot Q)) \nonumber \\
  & = & P \otimes (u \cdot Q) \nonumber
\end{eqnarray}

\subsubsection{Superposition as summation}

In this interpretation superposition corresponds to summation. That
is, if $P$ and $Q$ are going to represent states, then $P+Q$ represents
the superposition of those states. In terms of the evolution of
states, this choice is natural and intuitive. It also suggests a
definition for addition of scalars.

\begin{mathpar}
  x + y := \quotep{(\procn{x} \mathsf{+} \procn{y})}
\end{mathpar}

Intriguingly, in an interleaving-style operational semantics processes
correspond to the sum of all paths (sequences of actions), which would
give the usual identities for the distribution of multiplication and
addition.

\subsubsection{Evaluation}
The technique underlying the definitions for multiplication can be
extended to $\odot$ in the obvious manner, giving us the right to
write expressions like $x \odot P$, etc.

\subsubsection{Dirac notation}

Here we show the uncanny correspondence between the rho-calculus
operators and the basic operators of quantum mechanics in Dirac style
presentation and in general adopt a notation that emphasizes the roles
each object is playing.

\begin{table}[htp]
  \center{
    \fbox{
      \begin{tabular}{c|c}
        quantum mechanics & process calculus \\
        \hline
        $\ketp{P}$ & $P$ \\
        $\brap{P}$ & $P^{\bot}$ \\
        $\testp{P}{Q}$ & $P^{\bot} \cdot Q$\\
        $\ketp{P} \otimes \ketp{Q}$ & $P \otimes Q$\\
        $\ketp{P} \mathsf{+} \ketp{Q}$ & $P \mathsf{+} Q$ \\
      \end{tabular}
    }
  }
  \caption{QM - process calculi correspondences}
\end{table}

These definitions respect the well known identities. For example

\begin{eqnarray}
  \brap{P} (x\cdot \ketp{Q}) & = & (\brap{P} \cdot x) \ketp{Q} \nonumber
\end{eqnarray}

which allows us to write $\testnp{P}{x}{Q}$ unambiguously.

\subsubsection{The Born rule as an interpretation of non-determinism in the evolution of states}

The $\mathsf{COMM}$-rule clearly admits non-deterministic evolution of
states. Specifically, there are two basic forms of non-determinism
typically referred to as \emph{races} in the classical interpretation
of the rho-calculus. The first one is when there are more outputs than
there are inputs, and the second is when there are more inputs than
there are outputs. Symbolically,

\begin{mathpar}
  \prefix{x}{y_{1}}{P_{1}} \mathsf{|}\outputp{x}{Q}\mathsf{|} \prefix{x}{y_{2}}{P_{2}}
  \and
  \outputp{x}{Q_{1}} \mathsf{|}\prefix{x}{y}{P}\mathsf{|}\outputp{x}{Q_{2}}
\end{mathpar}

In the first case the two states reachable by reduction are
$P_{1}\substn{\quotep{Q}}{y_{1}}$ and
$P_{2}\substn{\quotep{Q}}{y_{2}}$; and in the second case the two
states reachable by reduction are $P\substn{\quotep{Q_{1}}}{y}$ and
$P\substn{\quotep{Q_{2}}}{y}$. In many implementations of the
rho-calculus, such as in the RChain blockchain, the interpretation of
this non-determinism is as an actual race. One of the communications
beats the other, either as an asynchronous communication over a
standard protocol (such as TCP), or one pair of threads are scheduled
to work over the other pair, in a threading library inside an
executing process.

In stochastic versions of these calculi the idea is to extend channels
with \emph{rates}. Thus,

\begin{mathpar}
  \prefix{x}{y}{P} \mapsto \prefix{(x,r)}{y}{P}
  \and
  \outputp{x}{Q} \mapsto \outputp{(x,r)}{Q}
\end{mathpar}

The rates are used to determine a 1-norm probability distribution over
the possible $\mathsf{COMM}$ events, at a given number of reduction
steps from the starting expression.

There is nothing in principle that prevents using 2-norm probability
distributions, where the rates are given by complex numbers and
contribute probability amplitudes. That's what we develop next.

\subsubsection{Continuous Time Markov Chains and their Quantum Variants}
\paragraph{Gillespie's method}
Gillespie's method \cite{ADS:ARPC/Gillespie2007} allows for an interpretation
of chemical equations with rates that affords a stochastic simulation
when the species populations are relatively constrained (as they are
in cells). The algorithm has been succesfully adapted to stochastic
versions of the $\pi$-calculus. The basis for this adaptation is the
intuition that chemical species, i.e. molecules, are processes, and
thus each $\mathsf{COMM}$ event corresponds the execution of a
reaction. To apply the method we must enrich the channel-based
reaction of the $\pi$-calculus with reaction rates. For example, in
the $\pi$-calculus process

\begin{mathpar}
  \prefix{x_{1}}{y_{1}}{P_{1}} + \prefix{x_{2}}{y_{2}}{P_{2}}
\end{mathpar}

represents a molecule that can react on two channels, $x_{1}$, and
$x_{2}$. Meanwhile,

\begin{mathpar}
  \outputp{x_{1}}{z_{1}}\mathsf{;}Q_{1}
  \and
  \outputp{x_{2}}{z_{2}}\mathsf{;}Q_{2}
\end{mathpar}

represent molecules that can react on $x_{1}$ and $x_{2}$,
respectively. In this context, a solution is a parallel composition of
a number of copies of each of the different kinds of
processes. Writing $[P]_{N}$ for $N$ copies of $P$ in parallel
composition, i.e. $P \mathsf{|} \cdots \mathsf{|} P$, then the general
form of a solution using the species of processes described above is

\begin{eqnarray}
  S = [\outputp{x_{1}}{z_{1}}\mathsf{;}Q_{1}]_{M_{1}} & \; \mathsf{|} \; [\prefix{x_{1}}{y_{1}}{P_{1}} + \prefix{x_{2}}{y_{2}}{P_{2}}]_{M_{2}} \; \mathsf{|} \; & [\outputp{x_{2}}{z_{2}}\mathsf{;}Q_{2}]_{M_{3}} \nonumber
\end{eqnarray}

If we fix

\begin{mathpar}
  \mathsf{rate}(x_{1}) = r_{1}
  \and
  \mathsf{rate}(x_{2}) = r_{2}
\end{mathpar}

then the probability of $S$ reducing via any interaction is $R =
\Sigma r_{i}M_{i}M_{i+1}$. And the probability of interacting via
$x_{i}$ is $r_{i}M_{i}M_{i+1}/R$. Having specified our interpretation
as a CMTC, we can apply Gillespie's algorithm to simulate how $S$
evolves.

The same method works for the rho-calculus. Note that the method is
oblivious to the structure of names, as it must be to interpret the
$\pi$-calculus, which hides all structure of names. However, there is
an intriguing option when using the annihilation-based version of
reduction. 

\paragraph{Xu, et al's method}
Xu, et al \cite{DBLP:journals/corr/abs-2105-00382} have developed an analog of Gillespie's method
for quantum continuous time Markov chains. We follow their
presentation closely.

\begin{definition}
  A quantum continuous Markov chain, $\mathcal{D}$, with a finite set
  $S$ of classical states is a pair $(\mathcal{H}_{cq},\mathcal{L})$,
  such that
  \begin{itemize}
     \item $\mathcal{H}_{cq} := \mathcal{C} \times \mathcal{H}$ where
       \begin{itemize}
          \item $\mathcal{C} := span(\{ \mathsf{|}s\rangle : s \in S\})$
          \item $\mathcal{H}$ a Hilbert space.
       \end{itemize}
     \item $\mathcal{L}$ is a transition generator function given by a
       Hermitian operator $\mathsf{H}$ and a finite set of linear
       operator, $\mathsf{L}_{j}$, on $\mathcal{H}_{cq}$.
  \end{itemize}
\end{definition}

We need a lemma allowing us to convert a CMTC to a QCMTC.


%% \subsubsection{Tensor product}

%% We define a tensor product on processes by structural induction.

%% \paragraph{Tensor of sums} First note that all summations, including
%% $\pzero$ and sequence, can be written $\Sigma_{i} x_{i}.A_{i} +
%% \Sigma_{j} x_{j}.C_{j}$, where we have grouped input-guarded processes
%% together and output-guarded processes together.

%% Thus, we can define the tensor product of two summations, $N_{1}\otimes N_{2}$, where

%% \begin{mathpar}
%%   N_{1} := \Sigma_{i} x_{i}.A_{i} + \Sigma_{j} x_{j}.C_{j}
%%   \and
%%   N_{2} := \Sigma_{i'} y_{i'}.B_{i'} + \Sigma_{j'} y_{j'}.D_{j'} 
%% \end{mathpar}

%% as follows.

%% \begin{mathpar}
%%   \Sigma_{i} x_{i}.A_{i} + \Sigma_{j} x_{j}.C_{j} \otimes \Sigma_{i'}
%%   y_{i'}.B_{i'} + \Sigma_{j'} y_{j'}.D_{j'} 
%%   \and \\
%%   := \; \Sigma_{i} \Sigma_{i'} \quotep{\stackrel{\vee}{x_{i}}| \stackrel{\vee}{y_{i'}}}.(A_{i}\otimes B_{i'}) \; | \; \Sigma_{i'} \Sigma_{i} \quotep{\stackrel{\vee}{y_{i'}}|\stackrel{\vee}{x_{i}}}.(B_{i'}\otimes A_{i})
%%   \and
%%   \;\; | \;\; \Sigma_{j} \Sigma_{j'} \quotep{\stackrel{\vee}{x_{j}}|\stackrel{\vee}{y_{j'}}}.(A_{j}\otimes B_{j'}) \; | \; \Sigma_{j'} \Sigma_{j} \quotep{\stackrel{\vee}{y_{j'}}|\stackrel{\vee}{x_{j}}}.(B_{j'}\otimes A_{j})
%% \end{mathpar}

%% \begin{remark}
%%   Do we need to $x^{L}$ and $y^{R}$ for this construction as well?
%% \end{remark}

%% \paragraph{Tensor of parallel compositions} Next, we distribute tensor
%% over par.

%% \begin{mathpar}
%%   P_{1}|P_{2} \otimes Q_{1}|Q_{2} := (P_{1} \otimes Q_{1}) | (P_{1}
%%   \otimes Q_{2}) | (P_{2} \otimes Q_{1}) | (P_{2} \otimes Q_{2})
%% \end{mathpar}

%% \paragraph{Tensor with dropped names} We treat tensor of a
%% process with a dropped name as parallel composition.

%% \begin{mathpar}
%%   P \otimes \dropn{x} := P | \dropn{x}
%% \end{mathpar}

%% \paragraph{Tensor of agents}

%% Finally, we need to define tensor on agents. Note that the definition
%% of tensor on summations only tensors inputs with inputs and outputs
%% with outputs. Thus, we only have to define the operation on
%% ``homogeneous'' pairings.

%% \begin{mathpar}
%%   (\arrvec{x})P \otimes (\arrvec{y})Q
%%   \and \\
%%   := (x_{0}^{L}|y_{0}^{R},\ldots,x_{0}^{L}|y_{n}^{R},\ldots,x_{m}^{L}|y_{0}^{R},\ldots,x_{m}^{L}|y_{n}^R)(P\{ \arrvec{x}^{L}/\arrvec{x}\} \otimes Q \{ \arrvec{y}^{R}/\arrvec{y}\})
%%   \and \\
%%   \clift{\arrvec{P}} \otimes \clift{\arrvec{Q}}
%%   \and \\
%%   := \clift{P_{0}\otimes Q_{0},\ldots,P_{0}\otimes Q_{n},\ldots,P_{m}\otimes Q_{0},\ldots,P_{m}\otimes Q_{n}}
%% \end{mathpar}

%% \begin{remark}
%%   Observe that arities of tensored abstractions matches arities of
%%   tensored concretions if the original arities matched. Note also that
%%   the length of the arities corresponds to the increase in dimension
%%   we see in ordinary vector space tensor product.
%% \end{remark}

%% \begin{remark}
%%   Operationally, this definition distributes the tensor down to
%%   components ``linked'' by summation. Tensor over summation is
%%   intriguing in that it mixes names. Moreover, as a consequence of the
%%   way it mixes names we have the identities for all $x \in \QProc$ and
%%   $P,Q \in \Proc$

%%   \begin{mathpar}
%%     (x \cdot P) \otimes Q \equiv x \cdot (P \otimes Q) \equiv P \otimes (x \cdot Q)
%%     \and \\
%%     P \otimes \pzero \equiv P
%%   \end{mathpar}

%%   that the reader is invited to verify.
%% \end{remark}

%% \subsubsection{Annihilation}
%% \begin{mathpar}
%%   P^{\perp} := \{ Q : \forall R. P|Q \red^{*} R \Rightarrow R \red^{*} \pzero \}
%%   \and \\
%%   \annihilate{P} := \Sigma_{Q \in P^{\perp}} \quotep{Q}?(y).(\dropn{y}|Q) | \Sigma_{Q \in P^{\perp}} \quotep{Q}\clift{\Box}
%% \end{mathpar}

%% \paragraph{Discussion} The reader will note that $P^{\perp}$ is a
%% \emph{set} of processes, while $\annihilate{P}$ is a
%% \emph{context}. We call the set $P^{\perp}$ the \emph{annihilators} of
%% $P$. The parallel composition of a process in the annihilators of $P$
%% with $P$ will result in a process, the state space of which has all
%% paths eventually leading to $\pzero$. Execution may endure loops; but
%% under reasonable conditions of fairness (naturally guaranteed under
%% most notions of bisimulation) such a composite process cannot get
%% stuck in such a loop and will, eventually pop out and terminate.

%% The context $\annihilate{P}$ is ready and willing to ``take the
%% $P$ out of'' the process to which it is applied. It will effectively
%% transmit the code of the process to which it is applied to one of the
%% annihilators and run the process against it.

%% \begin{remark}
%%   Note that ${\annihilate{P}}^*$ is the abstraction corresponding to
%%   context $\annihilate{P}$. We will set $\dualize{P} := {\annihilate{P}}^*$.
%% \end{remark}

%% \subsubsection{Evaluation}
%% We fix $M$ a domain of fully abstract interpretation with an equality
%% coincident with bisimulation. We take $\meaningof{\cdot} : \Proc \to
%% M$ to be the map interpreting processes and $\nmeaningof{\cdot} : \M
%% \to Proc$ to be the map running the other way. Then we define

%% \begin{mathpar}
%%   \int P := \nmeaningof{\meaningof{P}}
%% \end{mathpar}

%% \paragraph{Discussion}
%% There are many fully abstract interpretations of Milner's
%% $\pi$-calculus. Any of them can be used as a basis for interpreting
%% the reflective calculus here. Equipped with such a domain it is
%% largely a matter of grinding through to check that the Yoneda
%% construction for the normalization-by-evaluation program can be
%% extended to this setting.

%% \begin{remark}
%%   The reader is invited to verify that $\int (\annihilate{P}[P]) = 0$,
%%   and equivalently $(\nu\; x)\int \dualize{P}\langle x \rangle |
%%   x\clift{P} = 0$.
%% \end{remark}

%% \subsection{Quantum mechanics}

%% \subsubsection{What is the quantum mechanical notion of continuation?}\label{sec:quantum_continuation}

%% Imagine the following experimental set-up. Alice, our intrepid quantum
%% investigator, prepares a state by performing some operation on some
%% initial state. Then she performs some measurement to obtain an
%% observation. Using the information of the observation, she selects a
%% new initial state, operation and measurement and repeats the steps
%% above. She iterates this procedure until she obtains some desired
%% observation. What is the expression of this procedure in the language
%% of quantum mechanics?

%% \begin{figure}[htp]\label{fig:iterated_experiment}
%% %  \fbox{
%%     \begin{lstlisting}[mathescape]
%%       $\mathcal{E} ::=$
%%       let S = $U \state{L}$ in (* prepare state*)
%%       let m = $\innerprod{M}{S}^2$ in (* take measurement *)
%%       match m with (* use m to decide next experiment *)
%%       v$_0$ -> $\mathcal{E}$
%%       | $\ldots$
%%       | v$_N$ -> $\mathcal{E}$
%%       | v$_{Exit}$ -> m (* return observation *)
%%     \end{lstlisting}
%% %    }
%%   \caption{Iterated experiment schematic}
%% \end{figure}

%% Figure \ref{fig:iterated_experiment} gives a schematic description of
%% such an iterated experimental procedure. The question is how do we
%% write down this iterated procedure without stepping outside the
%% language of quantum mechanics? Note that accounts of famous composite
%% quantum experiments, like the Stern-Gerlach experiment leave the
%% language of quantum mechanics to describe the iterated experiment.

%% We ask this question for many reasons, but one of them is to help set
%% up the exegesis of our interpretation. In our framework
%% \emph{everything} is a computation, both the quantum operations and
%% processes (classical or quantum) that invoke those operations. There
%% is no need to step out of the conceptual (and more pragmatically, the
%% computational) framework to describe these kinds of experiments. More
%% to the point, the framework we are proposing is -- like the hybrid
%% functional language employed in the schema -- \emph{compositional}:
%% experiments, computations are built out of experiments and
%% computations. This is of enormous pragmatic value if we are to build
%% and reason about systems of significant scale.

%% \begin{remark}
%%   It is also worth noting in this connection that this schema is the
%%   core of a wide range of recursive functions. Further, this
%%   connection to calculations of fixpoints makes it a close neighbor of
%%   search techniques like natural selection and the scientific
%%   method. This is a theme to which we will return, for quantum
%%   information seems very \emph{unlife-like} in it's uncloneable,
%%   undeleteable nature.
%% \end{remark}

%% Returning the matter of the computational interpretation, our
%% interpretation will take the form of a map, written
%% $\meaningof{-}(-)$, from expressions in Dirac notation to expressions
%% in our target reflective calculus. The map takes an \emph{ancillary}
%% argument, a channel along which to communicate results to subsequent
%% computations. This is how we communicate, for example, the results of
%% taking a measurement to a subsequent step in an experiment.

%% \subsubsection{Interpretation}

%% Table \ref{tbl:core_qm_op_defns} gives the core operational
%% correspondences. It is meant as an intuitive guide.

%% \begin{table}[htp]\label{tbl:core_qm_op_defns}
%%   \center{
%%     \fbox{
%%       \begin{tabular}{c|c}
%%         quantum mechanics & process calculus \\
%%         \hline
%%         scalar & $x := \quotep{P}$ \\
%%         state vector & $\state{P} := P$ \\
%%         dual & $\state{P}^{*} := \event{\annihilate{P}} := \quotep{\annihilate{P}}[-]$ \\
%%         matrix & $ \Sigma_{\alpha} \state{P_{\alpha}}x_{\alpha}\event{Q_{\alpha}}$ \\
%%         vector addition & $\state{P} + \state{Q} := \state{P | Q}$ \\
%%         tensor product & $\state{P} \otimes \state{Q} := \state{P \otimes Q}$ \\
%%         inner product & $\innerprod{P}{Q} := \quotep{\int \annihilate{P}[Q]}$ \\
%%       \end{tabular}
%%     }
%%   }
%%   \caption{QM - operational definitions}
%% \end{table}

%% \paragraph{Discussion}
%% The process algebraic view of a state is called, ironically, a
%% process, and that is what we map vectors to in our interpretation. It
%% has long been noted in the process algebraic community that names play
%% a role somewhat similar to scalars in a vector space. What is unique
%% about the reflective calculus, and makes it suitable for an
%% interpretation of this form is that with the structure of names
%% reflecting the structure of processes we can both make this similarity
%% in a precision instrument; and find a notion of \emph{dual} to a
%% state. 

%% If we posit names as scalars, then in perfect analogy with vector
%% spaces a dual is a map from processes to names. We actually have two
%% candidates for this interpretation: nominal contexts, $\quotep{M}$,
%% and their corresponding abstraction, $\quotep{M}^{*}$. The goal of
%% supporting a notion of continuation selects the latter of the two for
%% our interpretation.

%% Taking these as the basis of the interpretation together with the
%% algebraic identities required by the Dirac notation more or less fixes
%% the definitions of the rest of the operations. Among the interesting
%% particularities, the definition of inner product finds near perfect
%% mirroring of the Feyman interpretation.

%% \begin{mathpar}
%%   \inferrule* [lab=states] {} {\meaningof{\state{P}}(c) = c?(l,r).r\clift{P}}
%%   \and
%%   \inferrule* [lab=events] {} {\meaningof{\event{P}}(c) = (x)c?(l,r).l\clift{\dualize{P}\langle x \rangle} } 
%%   \and
%%   \inferrule* [lab=vector addition] {} {\meaningof{\state{P} + \state{Q}}(c) = \meaningof{\state{P | Q}}(c)}
%%   \and
%%   \inferrule* [lab=tensor product] {} {\meaningof{\state{P} \otimes \state{Q}}(c) = \meaningof{\state{P\otimes Q}}(c)}
%%   \and
%%   \inferrule* [lab=inner product] {} {\meaningof{\innerprod{P}{Q}}(c) = (\nu\; x)c\clift{\int \dualize{P}\langle x \rangle | x\clift{Q}}}
%%   \and
%%   \inferrule* [lab=matrix] {} {\meaningof{\fprmatrix{P}{x}{Q}}(c) = \\\\ (u)(\nu \; lr) c!(l,r).(l?(e). (\nu\; y)\meaningof{\innerprod{\dropn{e}}{Q}}(x) | x?(z).x?(a).c\clift{(\dualize{P}\sigma(z,a))\langle u \rangle}|y!(x) \\\\
%%     + r?(e). (\nu\; x)\meaningof{\innerprod{P}{\dropn{e}}}(x) | x?(z).x?(a).c\clift{Q\sigma(z,a)}|x!(x_{\alpha}))}  
%%   \and
%%   \inferrule* [lab=matrix application] {} {\meaningof{(\fprmatrix{P}{x}{Q})(\state{S})}(c) = (\nu\; c'u)\meaningof{\fprmatrix{P}{x}{Q}}(c')\langle u \rangle | \meaningof{\state{S}}(c') | c'?(a).c!(a)
%%   \\\\
%%   \meaningof{(\fprmatrix{P}{x}{Q})(\event{S})}(c) = (\nu\; c'u)\meaningof{\fprmatrix{P}{x}{Q}}(c')\langle u \rangle | \meaningof{\event{S}}(c') | c'?(a).c!(a)}
%% \end{mathpar}

%% where

%% \begin{mathpar}
%%   P\sigma(z,a) := P\{ z\cdot a\cdot r/r : r \in \freenames{P} \}
%% \end{mathpar}

%% \begin{remark}
%%   The reader is invited to verify that
%%   \begin{mathpar}
%%     \meaningof{\innerprod{P}{Q}}(c)
%%     \and \\
%%     \wbbisim 
%%     \and \\
%%     %\and
%%     (\nu\; x)(\nu c'lr)\meaningof{\event{P}}(c')\langle x \rangle \;|\; \meaningof{\state{Q}}(c')
%%     %\and
%%     |\; c'!(l,r).l?(p).r?(q).c\clift{\int \dropn{p} | x!(q)}
%%   \end{mathpar}
%%   This provides a (more) compositional definition of inner product. It
%%   also illustrates an important point of the computational
%%   interpretation. We have a notion of equivalence providing a crucial
%%   proof method: bisimulation.
%% \end{remark}

%% \begin{remark}
%%   Assuming $\int (\annihilate{P}[P]) = 0$, the reader is
%%   invited to verify that $(\fprmatrix{P}{x}{P})(\state{P}) = x \cdot \state{P}$.
%% \end{remark}

%% % \begin{remark}
%% %   The reader is invited to verify that $\innerprod{P}{Q}$ could
%% %   equally well have been written $\quotep{\int \stackrel{\vee}{x}}$
%% %   where $x = \event{\annihilate{P}}(Q)$.

%% %   One of the motivations for this remark is that there is another way
%% %   to factor these operations. We could package up evaluation in the dual:

%% %   \begin{mathpar}
%% %     \state{P}^{*} := \event{\int \annihilate{P}} := \quotep{\int \annihilate{P}}[-]
%% %   \end{mathpar}

%% %   and then have inner product defined by
  
%% %   \begin{mathpar}
%% %     \innerprod{M}{Q} := \event{M}(Q)
%% %   \end{mathpar}

%% %   where we use $M$ to label the dual to emphasize that it is a context.

%% %   Hopefully, experience with the calculations will provide guidance on
%% %   the best factoring.
%% % \end{remark}

%% \begin{remark}\label{rem:abstract_scalars}
%%   Assuming $\int (\annihilate{P}[P]) = 0$, the reader is
%%   invited to verify that $\forall P,Q. (\prmatrix{0}{Q})(\state{0}) =
%%   \state{0}$ and dually $(\prmatrix{P}{0})(\event{0}) = \event{0}$.
%% \end{remark}

%% \subsubsection{Interpreting continuations}

%% As promised, we can combine these interpretations with standard
%% semantics for conditionals and continuations to provide an
%% interpretation of the iterated experiment.

%% \begin{lstlisting}[mathescape]      
%%   $\ldb$ let S = $U \state{L}$ in 
%%   let m = $\innerprod{M}{S}^2$ in 
%%   match m with 
%%   v$_0$ -> $\mathcal{E}$
%%   | $\ldots$
%%   | v$_N$ -> $\mathcal{E}$
%%   | v$_{Exit}$ -> m $\rdb(c)$
%%   $=$
%%   $(\nu\; c')\meaningof{\innerprod{M}{U\state{L}}}(c')$
%%   $| c'?(m).m!(m) | \Sigma_{i=0}^{N}v_{i}?(m).\meaningof{\mathcal{E}} + v_{Exit}?(m).c!(m)$
%% \end{lstlisting}

%% \paragraph{A quick tally}
%% Already the interpretation is beginning to show signs of
%% promise. First of all, it is no more notationally cumbersome than the
%% notation used in QM calculations. Beyond syntax, we have a new proof
%% principle in hand and the ability to reason about more complex
%% experimental situations than is directly calculable in ordinary
%% quantum mechanics.

%% \subsubsection{Adjointness}

%% We need to give a definition of $(\cdot)^{\dagger}$ for matrices. The
%% obvious candidate definition is
%% \begin{mathpar}
%% \meaningof{(\fprmatrix{P}{x}{Q})^{\dagger}}(c)
%% = (\nu\; u)\meaningof{\fprmatrix{\dualize{Q}\langle u \rangle}{\overline{x}}{\dualize{P}\langle u \rangle}}(c) 
%% \end{mathpar}

%% % But, $(Q_{\alpha}^{\underline{\perp}})^{*}$ requires a name along
%% % which to communicate the process to achieve the context application.

%% \begin{remark}
%%   i'm a little worried that i don't (yet) have proper support for
%%   complex conjugacy. But, the observation above may give us a
%%   clue. According to Abramsky, it must be the case that the scalars
%%   are iso to the homset of the identity for the tensor -- which the
%%   observation above (\ref{rem:abstract_scalars}) characterizes. For
%%   now, we will simply bookmark the notion with $\overline{x}$.
%% \end{remark}

%% \subsubsection{Basis for a basis}
%% If processes label states and ``addition'' of states (a.k.a. vector
%% addition) is interpreted as parallel composition, what corresponds to
%% notions of linear independence and basis? Here, we recall that Yoshida
%% has developed a set of \emph{combinators} for an asynchronous verison
%% of Milner's $\pi$-calculus \cite{DBLP:conf/concur/Yoshida98}. These
%% are a finite set of processes such any process can be expressed as
%% parallel composition of these combinators together with liberal uses
%% of the new operator and replication. We can simply give a translation
%% of these into the present calculus and have reasonable expectation
%% that the property carries over. That is, that the resultant set allows
%% to express all processes via parallel composition. Note, however, that
%% there is no new operator or replication in this calculus. As a result,
%% we expect that the corresponding set is actually infinite. That is, we
%% expect that the space is actually infinite dimensional.

%% \begin{remark}
%%   The reader familiar with the lambda calculus may reasonably object:
%%   certainly, the collection $S$, $K$ and $I$ is a finite set of
%%   combinators \cite{Barendregt84}. Shouldn't we expect to see a finite
%%   set of combinators for an effectively equivalent system? i am very
%%   sympathetic to this critique and feel it warrants full attention. On
%%   the other hand, i also have in mind the following analogy. The
%%   natural numbers, as a monoid under addition, has exactly $1$
%%   generator, while the natural numbers, as a monoid under
%%   multiplication, has countably many generators (the primes). We
%%   observe that the application of the lambda calculus is much less
%%   resource sensitive than the parallel composition of the
%%   $\pi$-calculus. Could it be the case that we have an analogy of the
%%   form
  
%%   \begin{mathpar}
%%     m + n : MN :: m*n : M|N
%%   \end{mathpar}

%%   giving a similar blow up in the set of ``primes''?  This is such a
%%   wonderful thought that, even if it's not true, i think it's worth
%%   writing down.
%% \end{remark}
