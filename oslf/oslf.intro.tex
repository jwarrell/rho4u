\section{Introduction}\label{sec:introduction} % (fold)

Programmers who work on large scale deployments, like Facebook,
Google, or Microsoft services, know that it is just not cost effective
to write code at scale in an untyped language. Why waste human time
and effort on writing tests (and debugging them) and worrying about
coverage for cases that a compiler can completely and exhaustively
automate? That is why the big three (Facebook, Google, and Microsoft)
all spent tens of millions of dollars each developing typed versions
of javascript. It's cheaper than continuing to throw good money
after bad on commercial development at industrial scale in an
untyped language. All told, the cost to the industry of this one issue
alone is in the hundreds of millions of dollars.

But, as we move deeper and deeper into the era of distributed
concurrent computing, the sorts of errors that mainstream programming
languages catch with their type systems are only the tip of the
iceberg. The Java type system cannot catch liveness errors. The Rust
type system cannot catch security errors. However, the programming
language semantics community has known for 30 years about techniques
for catching a much wider range of safety and liveness errors. Session
types, spatial and behavioral types are well explored areas bearing
considerable fruit on just these sorts of issues. And, just like with
javascript, it is taking the community ages to get these techniques
into the hands of front line developers.

What if it didn't have to be this way? What if the type system for
spatial and behavioral types could be algorithmically generated from
the specification of the programming language? That's what this paper
is about: an algorithm that takes as input a description of a model of
computation as a rewrite system and \emph{generates} as output a
spatial-behavioral type system for the model.

The formalization of the rewrite system uses Fiore's higher-order
abstract syntax to specify the syntax with binders. This is augmented
with a theory of graphs specifying the source and targets of the
rewrite rules. This yields what amounts to a Cartesian closed category
($\mathsf{CCC}$), call it $\mathcal{C}$. To $\mathcal{C}$ we apply the
following recipe. First we hit is with Yoneda, resulting in a category
of presheaves. We then hit that with a construction described below
turning the homs into complete Heyting algebras. Finally, we take the
Grothendieck construction. The result is that types are pairs
$(\mathbf{U}, \mathbf{X})$ consisting of a sort, $\mathbf{X}$, from
the original lambda theory and a filter on it, $\mathbf{U}$.

The resulting category, denoted by $\mathsf{NT}(\mathcal{C})$, enjoys
a rich logical structure (in fact, it is a topos) and thus provides a
stable means for combining collections of witnesses of various
properties in various ways. Specifically the homs are complete Heyting
algebras and hence enjoy that particular menu of lattice
operations. Additionally, there are products and sums, which can be
made to interact coherently with the lattice operations. Likewise, we
have two different forms of implication.

Further, because the rewrites have been explicitly encoded we get
typed versions of redex constructors, as well as typed versions of
reduction, and modal operators ala Hennessy-Milner logics, all of this
mechanism delivers a powerful behavioral typing
apparatus. Additionally, we find that there are typed versions of the
term constructors. These play in the role of spatial types. 

\subsection{Summary of contributions and outline of paper}

\subsubsection{Summary of contributions}
In principle, everything here could be derived by a careful analysis
of native types. However, the native types paper rushes to introduce
advanced features, like dependent types, while missing certain key
points. For example, the derived meta theory introduces new terms that
are not covered by the reduction relation of the
calculus. Specifically, the product and sum terms introduced have no
corresponding reductions and thus the type judgments presented in the
paper are effectively useless for real programs because they will
never reduce. Moreover, it is actually important to pay attention to
the fact that there are two forms of conjunctions, disjunctions, and
implication, mirroring the sort of structure typically found in
semantics for linear logic.

Fortunately, it is possible to expand the reduction relation in a
natural way to include the reduction of these terms. Additionally, one
of the main goals of this work has been to provide an algorithm
parametric in the kind of collections used to gather witnesses. Here
we illustrate that the use of Yoneda and $\mathsf{sub}$ can be
modified to provide a much wider range of collections, and yielding
logical connectives corresponding to the usual operations on those
collections.

Finally, we set up a discussion regarding the proper role and shape of
the derived meta theory. In native types this meta theory is decidedly
sequential. This means that the logic is at odds with computational
models like the $\pi$-calculus and the rho-calculus that are
\emph{concurrent}. What is needed is an analog of the topos that is
fundamentally concurrent, a rho-pos, if you will. This is because
concurrency is more fundamental that sequentiality.

\subsubsection{Outline of paper}
First we provide an intuitive motivating example. Then we review Fiore's higher order abstract syntax as a mechanism for providing a categorical model for syntax with binders. To this we add a theory of graphs to capture the operational semantics. Then we review the native types construction in more detail.

The review out of the way we present the algorithm. Then illustrate it
by deriving a type system for the core of rholang. We prove a
substitutability theorem. We consider some examples types. Finally we
conclude with some future directions for research.

% section introduction (end)
