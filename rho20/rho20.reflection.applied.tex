\section{Applied reflection: Reflective set theory and reflective graphs}
In this section we illustrate, albeit briefly, that reflection has a
much wider utility than just computation. We describe reflective
versions of set theory and graph theory and illustrate via some core
examples how reflection accounts for an interesting range of phenomena.

\subsection{Reflective set theory}

This presentation will be just a sketch. A more rigorous formal
account is most likely built using Awodey's algebraic set theory.

\begin{mathpar}
  \inferrule* [lab=red-set] {} {\textcolor{red}{\mathsf{S}}[A] \bc \textcolor{red}{\bold{\mathsf{\{}}} (A \bm \textcolor{red}{\mathsf{S}}[A])^{*} \textcolor{red}{\bold{\mathsf{\}}}}}
  \and
  \inferrule* [lab=black-set] {} {\mathsf{S}[A] \bc \bold{\mathsf{\{}} (A \bm \mathsf{S}[A])^{*} \bold{\mathsf{\}}}} \\
  \and
  \inferrule* [lab=rset-player] {} {\mathsf{RSet}_{player} = \textcolor{red}{\mathsf{S}}[\mathsf{S}[\mathsf{RSet}_{player}]]}
  \and
  \inferrule* [lab=rset-opponent] {} {\mathsf{RSet}_{opponent} = \mathsf{S}[\textcolor{red}{\mathsf{S}}[\mathsf{RSet}_{opponent}]]}
  \and
  \inferrule* [lab=rset] {} {\mathsf{RSet} = \mathsf{RSet}_{player} \oplus \mathsf{RSet}_{opponent}}
\end{mathpar}

The intuition is that we have two distinct \emph{copies} of set
theory, which we will denote for purposes of discussion, red set
theory and black set theory. Both of these theories are considered
parametric in a theory of atoms. Thus, these theories are instances of
a Fraenkl-Mostowski set theory ($\mathsf{FM}$ set theory)
\cite{DBLP:journals/tcs/Gabbay09}. But, now we make the atoms of the
red set theory be black sets, and the atoms of the black set theory be
red sets.

Again, computer science provides a unique perspective on these age old
notions. The idea of copies of a theory comes about when we reify meta
theory as theory, something implicit in Awodey's work. Computer
science also provides a very practical means for realizing it: we have
different constructors (red braces versus black braces); and different
accessors (a red element-of operation versus a black element of
operation). Further, computer science is quite comfortable with these
kinds of mutually recursive structures. The mutual recursion bottoms
out in the red empty set and the black empty set.

Now, the presentation above is only suggestive because Kleene star
would only generate sets with finite cardinality. This is why Awodey's
algebraic set theory is the better formalism to adapt to formalize
these intuitions. But, from the perspective of computer science this
presentation very much matches how it might be coded up.

What motivates such a strange construction? It turns out that
$\mathsf{FM}$ set theory has been successfully applied to reasoning
about nominal phenomena, such as binders in the $\pi$-calculus. But,
once again, one is left with a dissatisfying theory of atoms without
structure. This theory explains where atoms come from and how they can
have exactly the same structure as sets. In other words, it reconciles
$\mathsf{FM}$ set theory with $\mathsf{ZF}$ set theory.

\subsection{Reflective graphs}

\subsection{Well-formedness}

This is an algebraic theory, and thus it is syntactic in nature. It
provides a language of graphs that are built out of logical sentences.
In more detail, the theory admits three kinds of sentences:

\begin{itemize}
\item
  recognizing an admissible vertex:
  \(\mathsf{G}[X,V]; \Gamma \vdash v\);
\item
  recognizing an admissible variable:
  \(\mathsf{G}[X,V]; \Gamma \vdash x\);
\item
  and, recognizing a well formed graph:
  \(\mathsf{G}[X,V]; \Gamma \vdash g\)
\end{itemize}

Variables are used to capture references to vertices which are in turn
used to form edges between vertices. As such, judging the wellformedness
of a graph depends on the use of references. Hence, a judgement makes
use of a dependency list, \(\Gamma\) which is just a sequence of
variables. That is,

\[\Gamma ::= () \;|\; x, \Gamma\]

Notationally, we overload the comma to indicate concatenation of
sequences. Thus, given \(\Gamma_1 = x_{11},\ldots,x_{1m}\) and
\(\Gamma_2 = x_{21},\ldots,x_{2n}\), then
\(\Gamma_1,\Gamma_2 = x_{11},\ldots,x_{1m},x_{21},\ldots,x_{2n}\)

A graph expression is given by the grammar

\[g,h ::= 0 \;|\; v|g \;|\; x|g \;|\; g \otimes h \; |\;\mathsf{let}\; x = v \; \mathsf{in}\; g \;|\; \langle \mathsf{let}\; x_1 = v_1 \; \mathsf{in}\; g, \mathsf{let}\; x_2 = v_2 \; \mathsf{in}\; h\rangle\]

\hypertarget{pronunciation}{%
\subsubsection{Pronunciation}\label{pronunciation}}

The graph constructors are pronounced as follows.

\begin{itemize}
\item
  \(0\) -- ``the empty graph'', or just ``empty'';
\item
  \(v | g\) -- ``\(g\) with the vertex \(v\) adjoined'', or ``adjoin
  \(v\) to \(g\)'';
\item
  \(g_1 \otimes g_2\) -- ``the graph formed by juxtaposing \(g_1\) and
  \(g_2\)'', or ``juxtapose \(g_1\) and \(g_2\)'', or just ``\(g_1\) and
  \(g_2\)'';
\item
  \(\mathsf{let}\; x = v \; \mathsf{in}\; g\) -- ``let \(x\) stand for
  \(v\) in \(g\)''; or ``let \(x\) be \(v\) in \(g\)'';
\item
  \(\langle \mathsf{let}\; x_1 = v_1 \; \mathsf{in}\; g_1,\mathsf{let}\; x_2 = v_2 \; \mathsf{in}\; g_2\rangle\)
  -- ``the graph formed by connecting \(g_1\) to \(g_2\) with and edge
  from \(x_1\) to \(x_2\)''; or, ``connect \(g_1\) to \(g_2\) with and
  edge from \(x_1\) to \(x_2\)''.
\end{itemize}

We say $x$ is fresh in $g$ if $x$ does not occur in $g$.

The judgment \(\mathsf{G}[X,V]; \Gamma \vdash g\) is pronounced
``\(\mathsf{G}[X,V]\) thinks that \(g\) is well-formed, given
dependencies, \(\Gamma\) .'' Similarly,
\(\mathsf{G}[X,V]; \Gamma \vdash v\) is pronounced ``\(\mathsf{G}[X,V]\)
thinks that \(v\) is an admissible vertex, given dependencies,
\(\Gamma\);'' and \(\mathsf{G}[X,V]; \Gamma \vdash x\) is pronounced
``\(\mathsf{G}[X,V]\) thinks that \(x\) is an admissible variable, given
dependencies, \(\Gamma\).''

A rule of the form

\[\frac{ H_1, \ldots , H_n }{ C }R\]

is pronounced ``\(R\) concludes that \(C\) given \(H_1\), \(\ldots\),
\(H_n\)''.

The rules for judging when a vertex or a variable are admissible or
graph is well formed are as follows

\[\frac{ }{ \mathsf{G}[X,V]; () \vdash 0}Foundation\]

\[\frac{ v \in V }{ \mathsf{G}[X,V]; () \vdash v}Verticity \;\;\frac{ x \in X }{ \mathsf{G}[X,V]; \emptyset \vdash x}Variation\]

\[\frac{ \mathsf{G}[X,V]; \Gamma \vdash g \;\; \mathsf{G}[X,V]; () \vdash v }{ \mathsf{G}[X,V]; \Gamma \vdash v | g}Participation \; \; \frac{ \mathsf{G}[X,V]; \Gamma \vdash g \;\; \mathsf{G}[X,V]; () \vdash x }{ \mathsf{G}[X,V]; \Gamma, x \vdash x | g}Dependence\]

\[\frac{ \mathsf{G}[X,V]; \Gamma_1 \vdash g_1 \; \mathsf{G}[X,V]; \Gamma_2 \vdash g_2}{ \mathsf{G}[X,V]; \Gamma_1, \Gamma_2 \vdash g_1 \otimes g_2}Juxtaposition[\Gamma_1 \cap \Gamma_2 = \emptyset]\]

\[\frac{ \mathsf{G}[X,V]; \Gamma,x \vdash v|g}{ \mathsf{G}[X,V]; \Gamma \vdash \mathsf{let}\; x = v \; \mathsf{in}\; g}Nomination[x \;fresh\; in\; g]\]

\[\frac{ \mathsf{G}[X,V]; \Gamma_1 \vdash \mathsf{let}\; x_1 = v_1 \; \mathsf{in}\; g_1 \; \;\mathsf{G}[X,V]; \Gamma_2 \vdash \mathsf{let}\; x_2 = v_2 \; \mathsf{in}\; g_2}{ \mathsf{G}[X,V]; \Gamma_1,\Gamma_2 \vdash \langle \mathsf{let}\; x_1 = v_1 \; \mathsf{in}\; g_1, \mathsf{let}\; x_2 = v_2 \; \mathsf{in}\; g_2 \rangle}Connection[\Gamma_1 \cap \Gamma_2 = \emptyset]\]

\subsubsection{Where's the reflection?}

As with all the other formalism put forward in this paper, this
account of graphs begins with a two-level type
decomposition. Specifically, it acknowledges that as a type, the type
of graphs is dependent on the type of vertices and edges. But, once
this dependency is acknowledged, we can readily form and solve fixed
point equations. Such as

\begin{mathpar}
  \mathsf{R}_{V} = \mathsf{G}[X,1 + \mathsf{R}_{V}] \\
\end{mathpar}

In fact, it is also possible to do something similar for edges.


\subsection{Reflection as integration}

It is common to think of the derivative of a polynomial functor as
punching a hole in the data type, that is, for a given functor,
$\mathsf{T}$, the functor $\partial\mathsf{T}$ is the type of 1-holed
contexts of $\mathsf{T}$. By the same token, we can think of
reflection as a kind of reflection in the sense that it is plugging
holes in a functor. Specifically, if $\mathsf{T}[X]$ is parametric in
$X$, then $\mathsf{R}(\mathsf{T}) =
\mathsf{T}[\mathsf{R}(\mathsf{T})]$ represents the elimination of that
dependency.

Unfortunately, a duality between derivation and reflection supporting
an analog of the fundamental theorem of calculus is not so easily
obtained. In particular, it is not generally the case that we have
$\mathsf{R}(\partial \mathsf{T}) = \mathsf{T}$. It would be of
interest to find conditions under which this does hold.
